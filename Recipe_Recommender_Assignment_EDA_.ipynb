{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6360575b",
      "metadata": {
        "id": "6360575b"
      },
      "source": [
        "**Recipe Recommender Assignment EDA**\n",
        "\n",
        "Submission Date Feb 4th\n",
        "\n",
        "Prasanth Rai\n",
        "\n",
        "Bhanu Teja Mantri\n",
        "\n",
        "Rahul Birhade\n",
        "\n",
        "\n",
        "# Recipe Recommender EDA\n",
        "\n",
        "This notebook contains the code and analysis for the Recipe Recommender project. It includes:\n",
        "1. **Data Reading and Processing:** Reading and cleaning raw recipe and interaction data.\n",
        "2. **Feature Extraction:** Extracting relevant features for modeling.\n",
        "3. **Exploratory Data Analysis:** Analyzing the extracted features to gain insights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "640bd3f7",
      "metadata": {
        "id": "640bd3f7"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "id": "04ed2518",
      "metadata": {
        "id": "04ed2518"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "id": "494d9d27",
      "metadata": {
        "id": "494d9d27"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Basics\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "id": "5a3d6550",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "5a3d6550",
        "outputId": "d557e08c-452c-48eb-cef7-0c94b1a19652"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7cbbcade1350>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ac3a956600d9:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Basics</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "id": "d956d7bf",
      "metadata": {
        "id": "d956d7bf"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Import for typecasting columns\n",
        "from pyspark.sql.types import IntegerType,BooleanType,DateType,FloatType,StringType\n",
        "\n",
        "from pyspark.sql.types import ArrayType"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "652d9b65",
      "metadata": {
        "id": "652d9b65"
      },
      "source": [
        " ## <font color='red'>Task 01: Read the data </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45cb47b",
      "metadata": {
        "id": "d45cb47b"
      },
      "source": [
        "<font color='red'> Ensure you read the data so that all columns are read with the right data type.\n",
        "The \"right\" datatype at this stage are shown in the expected output cell below. </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4ef807f",
      "metadata": {
        "id": "d4ef807f"
      },
      "source": [
        "<font color='red'>\n",
        "    \n",
        "**Sample input:**\n",
        "This task does not have an input.\n",
        "\n",
        "**Sample output:**\n",
        "Dataframe stored in the variable ```raw_recipes_df```.  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a2f1588",
      "metadata": {
        "id": "2a2f1588"
      },
      "source": [
        "![task1.png](attachment:task1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b0c21a",
      "metadata": {
        "id": "c0b0c21a"
      },
      "source": [
        "<font color='red'> We have included some test cases given below. You can use them to check if you have completed the task correctly.  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8a30e9",
      "metadata": {
        "id": "ad8a30e9"
      },
      "source": [
        "### <font color='blue'>Solution to Task 1 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4fe11a3",
      "metadata": {
        "id": "d4fe11a3"
      },
      "source": [
        "<font color='blue'>complete the code in the following cell </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "id": "e20dae66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e20dae66",
        "outputId": "d8c6f4de-ff67-4d8a-d902-fb230d673001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+-------+--------------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-------------+\n",
            "|                name|    id|minutes|contributor_id| submitted|                tags|           nutrition|n_steps|               steps|         description|         ingredients|n_ingredients|\n",
            "+--------------------+------+-------+--------------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-------------+\n",
            "|arriba   baked wi...|137739|     55|         47892|2005-09-16|['60-minutes-or-l...|[51.5, 0.0, 13.0,...|     11|['make a choice a...|autumn is my favo...|['winter squash',...|            7|\n",
            "|a bit different  ...| 31490|     30|         26278|2002-06-17|['30-minutes-or-l...|[173.4, 18.0, 0.0...|      9|['preheat oven to...|this recipe calls...|['prepared pizza ...|            6|\n",
            "|all in the kitche...|112140|    130|        196586|2005-02-25|['time-to-make', ...|[269.8, 22.0, 32....|      6|['brown ground be...|this modified ver...|['ground beef', '...|           13|\n",
            "|  alouette  potatoes| 59389|     45|         68585|2003-04-14|['60-minutes-or-l...|[368.1, 17.0, 10....|     11|['place potatoes ...|this is a super e...|['spreadable chee...|           11|\n",
            "|amish  tomato ket...| 44061|    190|         41706|2002-10-25|['weeknight', 'ti...|[352.9, 1.0, 337....|      5|['mix all ingredi...|my dh's amish mot...|['tomato juice', ...|            8|\n",
            "+--------------------+------+-------+--------------+----------+--------------------+--------------------+-------+--------------------+--------------------+--------------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Task 01 Cell 1 out of 1\n",
        "\n",
        "raw_recipes_df = spark.read.csv(\"RAW_recipes_cleaned.csv\",inferSchema=True,header=True)\n",
        "                   # argument 1, Add an argument to communicate to the compiler that there is a header in the raw data.\n",
        "                   # argument 2, Add an argument to ask the complier to estimate the data types for all columns.\n",
        "raw_recipes_df.show(5)\n",
        "# Please forward the exact name of data frames and columns as suggested in the code.\n",
        "# It will ensure that the assert commands function correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "id": "fc387a5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc387a5f",
        "outputId": "add52894-8a16-4f9b-f116-40364479e3ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- id: integer (nullable = true)\n",
            " |-- minutes: integer (nullable = true)\n",
            " |-- contributor_id: integer (nullable = true)\n",
            " |-- submitted: date (nullable = true)\n",
            " |-- tags: string (nullable = true)\n",
            " |-- nutrition: string (nullable = true)\n",
            " |-- n_steps: integer (nullable = true)\n",
            " |-- steps: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- ingredients: string (nullable = true)\n",
            " |-- n_ingredients: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "raw_recipes_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62407b85",
      "metadata": {
        "id": "62407b85"
      },
      "source": [
        "<font color='blue'>Test cases for Task 01</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "id": "a5b8d8b3",
      "metadata": {
        "scrolled": true,
        "id": "a5b8d8b3"
      },
      "outputs": [],
      "source": [
        "# Code check cell\n",
        "# Do not edit cells with assert commands\n",
        "# If an error is shown after running this cell, please recheck your code.\n",
        "\n",
        "assert raw_recipes_df.count() == 231637, \"There is a mistake in reading the data.\"\n",
        "assert len(raw_recipes_df.columns) == 12, \"There is a mistake in reading the data.\"\n",
        "assert raw_recipes_df.schema[\"minutes\"].dataType == IntegerType(), \"The data types have not been read correctly.\"\n",
        "assert raw_recipes_df.schema[\"tags\"].dataType == StringType(), \"The data types have not been read correctly.\"\n",
        "assert raw_recipes_df.schema[\"n_ingredients\"].dataType == IntegerType(), \"The data types have not been read correctly.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "id": "55534502",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55534502",
        "outputId": "fa7f618d-bbb4-43fb-99f9-853922f3dcbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ],
      "source": [
        "raw_recipes_df.count() == 231637"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "id": "ea589fa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea589fa3",
        "outputId": "57c731c1-cbb5-4db1-e63a-316002b7292b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ],
      "source": [
        "len(raw_recipes_df.columns) == 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "id": "5fa028de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fa028de",
        "outputId": "c9f32d33-38c9-40d2-89c4-8a591d2d94a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ],
      "source": [
        "raw_recipes_df.schema[\"minutes\"].dataType == IntegerType()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "id": "f907800c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f907800c",
        "outputId": "9a4f1d10-de61-4e09-afb8-c152e13113a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ],
      "source": [
        "raw_recipes_df.schema[\"tags\"].dataType == StringType()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "id": "19173364",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19173364",
        "outputId": "c7520575-f204-4e11-b140-349ee56ab6b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ],
      "source": [
        "raw_recipes_df.schema[\"n_ingredients\"].dataType == IntegerType()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abbd80be",
      "metadata": {
        "id": "abbd80be"
      },
      "source": [
        " #### <font color='red'>If all test cases pass task 01 ends </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d240566",
      "metadata": {
        "id": "8d240566"
      },
      "source": [
        "## Extract ```nutrition``` values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "id": "e60b6807",
      "metadata": {
        "id": "e60b6807"
      },
      "outputs": [],
      "source": [
        "# List of nutrition columns\n",
        "\n",
        "nutrition_column_names = ['calories',\n",
        "                          'total_fat_PDV',\n",
        "                          'sugar_PDV',\n",
        "                          'sodium_PDV',\n",
        "                          'protein_PDV',\n",
        "                          'saturated_fat_PDV',\n",
        "                          'carbohydrates_PDV']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383e4450",
      "metadata": {
        "id": "383e4450"
      },
      "source": [
        " ## <font color='red'>Task 02: Extract individual features from the nutrition column."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a836d795",
      "metadata": {
        "id": "a836d795"
      },
      "source": [
        "<font color='red'>\n",
        "As read by the spark compiler, the nutrition column is a string column when it should be an array of float values. Each row in the nutrition column contains seven values. Each value represents nutrition information.\n",
        "    \n",
        "    \n",
        "**Your task is to separate the array into seven individual columns.**\n",
        "    \n",
        "Write a code that takes in the nutrition column from ```raw_recipes_df``` dataframe, and extracts individual values into seven different columns named calories, total fat (PDV), sugar (PDV), sodium (PDV), protein (PDV), saturated fat (PDV), and carbohydrates (PDV).\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d47c797",
      "metadata": {
        "id": "7d47c797"
      },
      "source": [
        "<font color='red'>\n",
        "    \n",
        "### **Sample input:**\n",
        "    \n",
        "The image below shows a subset of columns from the ```raw_recipes_df``` dataset. The datatype of the nutrition column is a string.\n",
        " </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3c1e1cb",
      "metadata": {
        "id": "b3c1e1cb"
      },
      "source": [
        "![task2.01.png](attachment:task2.01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df30c694",
      "metadata": {
        "id": "df30c694"
      },
      "source": [
        "<font color='red'>\n",
        "    \n",
        "### **Sample Output:**\n",
        "    \n",
        "The image below shows a subset of columns from the ```raw_recipes_df``` dataset after the extraction of nutrition values is completed. The datatype of the individual nutrition column is has to be float.\n",
        " </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f684f24f",
      "metadata": {
        "id": "f684f24f"
      },
      "source": [
        "![task2.03.png](attachment:task2.03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5240146f",
      "metadata": {
        "id": "5240146f"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "This task is further divided into two sub tasks\n",
        "    \n",
        "### Task 2.1\n",
        "    \n",
        "Use string operations to remove the square brackets from the nutrition column.\n",
        "\n",
        "Sample input: nutrition column\n",
        "    </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19ac3c26",
      "metadata": {
        "id": "19ac3c26"
      },
      "source": [
        "![task2.01.png](attachment:task2.01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f221977",
      "metadata": {
        "id": "5f221977"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "\n",
        "**Sample output:**\n",
        "Nutrition column without the brackets.\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7af331b0",
      "metadata": {
        "id": "7af331b0"
      },
      "source": [
        "![task2.02.png](attachment:task2.02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39bdc7a5",
      "metadata": {
        "id": "39bdc7a5"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "### Task 2.2\n",
        "    \n",
        "Task 2.2 Split the nutrition column into seven individual columns and cast the new columns to float values.\n",
        "\n",
        "First split the column on using the comma delimiter. Then you can use a for loop to iterate over the column names declared in the variable ```nutrition_column_names```, inside each iteration write a code to extract the value at a specific index of the nutrition array  \n",
        "\n",
        "**Sample input:**\n",
        "Nutrition column without the brackets.\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31435271",
      "metadata": {
        "id": "31435271"
      },
      "source": [
        "![task2.02.png](attachment:task2.02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4655f67c",
      "metadata": {
        "id": "4655f67c"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "**Sample output:**\n",
        "Nutrition column split into multiple\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "884e23e8",
      "metadata": {
        "id": "884e23e8"
      },
      "source": [
        "![task2.03.png](attachment:task2.03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1c05c0",
      "metadata": {
        "id": "7b1c05c0"
      },
      "source": [
        "<font color='red'> We have included some test cases given below. You can use them to check if you have completed the task correctly.  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da588328",
      "metadata": {
        "id": "da588328"
      },
      "source": [
        "### <font color='blue'>Solution to Task 2 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c51ea51c",
      "metadata": {
        "id": "c51ea51c"
      },
      "source": [
        "<font color='blue'>complete the code in the following cell </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "id": "1083715f",
      "metadata": {
        "id": "1083715f"
      },
      "outputs": [],
      "source": [
        "# Task 02 Cell 1 out of 2\n",
        "# 2.1 - string operations to remove square brakets\n",
        "\n",
        "raw_recipes_df = (raw_recipes_df\n",
        "                  .withColumn('nutrition',(F.regexp_replace(\"nutrition\",\"[\\[\\]]\",\"\"))\n",
        "                             # add code to remove square brackets\n",
        "                             # pyspark function to replace string characters\n",
        "                             ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "id": "7c2a3cc8",
      "metadata": {
        "id": "7c2a3cc8"
      },
      "outputs": [],
      "source": [
        "# Task 02 Cell 2 out of 3\n",
        "# STEP 2.2 - split the neutrition string into seven individial values.\n",
        "# Create an object to split the nutrition column\n",
        "import pyspark\n",
        "nutrition_cols_split = pyspark.sql.functions.split(raw_recipes_df['nutrition'],',')\n",
        "\n",
        "# Write a loop to extract individual values from the nutrition column\n",
        "\n",
        "for col_index, col_name in enumerate(nutrition_column_names):\n",
        "    # col_index holds the index number of each column, e.g., calories will be 0\n",
        "    # col_name holds the name of each column\n",
        "\n",
        "    raw_recipes_df = (raw_recipes_df.withColumn(col_name, nutrition_cols_split.getItem(col_index).cast(\"float\")\n",
        "                                        # pyspark function to extract individual values from the nutrition_cols_split object\n",
        "                                        # You can also cast the extracted value to floats in the same code.\n",
        "                                               ))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "056898fa",
      "metadata": {
        "id": "056898fa"
      },
      "source": [
        "Hint: [Visit this page to learn more about splitting columns](https://sparkbyexamples.com/pyspark/pyspark-split-dataframe-column-into-multiple-columns/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4763390d",
      "metadata": {
        "id": "4763390d"
      },
      "source": [
        "**Test cases for task 02**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "id": "7323e7ff",
      "metadata": {
        "scrolled": true,
        "id": "7323e7ff"
      },
      "outputs": [],
      "source": [
        "# Code check cell\n",
        "# Do not edit cells with assert commands\n",
        "# If an error is shown after running this cell, please recheck your code.\n",
        "\n",
        "assert raw_recipes_df.schema[\"carbohydrates_PDV\"].dataType == FloatType(), \"Recheck your typecasting\"\n",
        "assert raw_recipes_df.collect()[123432][14] == 62.0, \"The columns have not been split correctly.\"\n",
        "assert raw_recipes_df.collect()[10000][12] == 60.400001525878906, \"The columns have not been split correctly.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "id": "0087685a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0087685a",
        "outputId": "3620821f-6593-40f4-e4a5-e908b8fefaf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ],
      "source": [
        "raw_recipes_df.schema[\"carbohydrates_PDV\"].dataType == FloatType()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "2026b621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2026b621",
        "outputId": "bb0ad088-30e5-4b65-97ab-9356f8ab0cfa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ],
      "source": [
        "raw_recipes_df.collect()[123432][14] == 62.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "id": "d09c78e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d09c78e9",
        "outputId": "8ad17ef7-cef0-4f3f-8214-7f478d47f611"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ],
      "source": [
        "raw_recipes_df.collect()[10000][12] == 60.400001525878906"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2c7c87c",
      "metadata": {
        "id": "e2c7c87c"
      },
      "source": [
        "#### <font color='red'>If all test cases pass task 02 ends </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eb0277c",
      "metadata": {
        "id": "0eb0277c"
      },
      "source": [
        "## Make nutrition-per-100 calorie columns\n",
        "\n",
        "By converting the nutrition values from absolute to relative terms, we ensure that portion size is not a factor in the analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ac526a5",
      "metadata": {
        "id": "8ac526a5"
      },
      "source": [
        "Naming convention: Original column name ```total fat (PDV)```, column name after column ```total_fat_per_100_cal```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55662bc0",
      "metadata": {
        "id": "55662bc0"
      },
      "source": [
        "## <font color='red'>Task 03: Standardize the nutrition values </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28dd54ed",
      "metadata": {
        "id": "28dd54ed"
      },
      "source": [
        "<font color='red'>\n",
        "The current values for nutrition columns are not on the same scale.\n",
        "Your task is to standardize the nutrition columns using calories as the base of standardization.\n",
        "\n",
        "Convert the nutrition from absolute values to per 100 calorie values.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40ca70f8",
      "metadata": {
        "id": "40ca70f8"
      },
      "source": [
        "<font color='red'>\n",
        "    \n",
        "We will use the  ```sugar (PDV)``` column to demonstrate the calculations for standardization.  \n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "892a8590",
      "metadata": {
        "id": "892a8590"
      },
      "source": [
        "![task3.01.png](attachment:task3.01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22596e49",
      "metadata": {
        "id": "22596e49"
      },
      "source": [
        "<font color='red'>\n",
        "    \n",
        "**Sample Calculation**\n",
        "\n",
        "Before transformation: ```sugar (PDV)``` for recipe id 137739 = 13.0\n",
        "\n",
        "Calories in the recipe recipe id 137739                       = 51.5\n",
        "\n",
        "Calculation:  \n",
        "sugar_per_100_cal = 13.0 * 100 / 51.5\n",
        "\n",
        "After transformation ```sugar_per_100_cal``` = 25.24\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3347a9de",
      "metadata": {
        "id": "3347a9de"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "**Sample Input:**\n",
        "\n",
        "All nutrition columns except calories\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92ffc6fe",
      "metadata": {
        "id": "92ffc6fe"
      },
      "source": [
        "![task3.02.png](attachment:task3.02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fff4ea7",
      "metadata": {
        "id": "2fff4ea7"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "**Sample Output:**\n",
        "\n",
        "All nutrition columns standardized to per 100 calories\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3306ed5",
      "metadata": {
        "id": "d3306ed5"
      },
      "source": [
        "![task3.03.png](attachment:task3.03.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b542d1c1",
      "metadata": {
        "id": "b542d1c1"
      },
      "source": [
        "<font color='red'> We have included some test cases given below. You can use them to check if you have completed the task correctly.  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3affae2f",
      "metadata": {
        "id": "3affae2f"
      },
      "source": [
        "### <font color='blue'>Solution to Task 3 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56ee388a",
      "metadata": {
        "id": "56ee388a"
      },
      "source": [
        "<font color='blue'>Complete the code in the following cell</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "id": "237faab1",
      "metadata": {
        "id": "237faab1"
      },
      "outputs": [],
      "source": [
        "# Task 03 Cell 1 out of 1\n",
        "\n",
        "for nutrition_col in nutrition_column_names:# loop over each of the newly created nutrition columns\n",
        "    if nutrition_col != \"calories\":\n",
        "        # the calories column should not be a part of the transformation exercise\n",
        "        # following code will name the new columns\n",
        "        nutrition_per_100_cal_col = (nutrition_col\n",
        "                                 .replace('_PDV','')\n",
        "                                 +'_per_100_cal')\n",
        "        raw_recipes_df = raw_recipes_df.withColumn(nutrition_per_100_cal_col,\n",
        "                                               raw_recipes_df[nutrition_col]*100/raw_recipes_df[\"calories\"]\n",
        "                                                # pyspark code to recreate the intended transformation\n",
        "                                                  )\n",
        "\n",
        "        # You might end up adding nulls to the data because of our intended transformation.\n",
        "        # Perform a fill na operation to fill all the nulls with 0s.\n",
        "        # You must limit the scope of the fill na to the current column only.\n",
        "\n",
        "        raw_recipes_df = raw_recipes_df.fillna(value=0,subset=[nutrition_per_100_cal_col])\n",
        "        # pyspark code to fill nulls with 0 in only the current nutrition_per_100_cal_col"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2656a22",
      "metadata": {
        "id": "d2656a22"
      },
      "source": [
        "**Test cases for Task 03**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "id": "8a97ad4c",
      "metadata": {
        "id": "8a97ad4c"
      },
      "outputs": [],
      "source": [
        "# total fat check for id 28881\n",
        "assert raw_recipes_df.filter(\"id == 28881\").select('total_fat_per_100_cal').first()[0] == 0, \"total_fat_per_100_cal for recipe 28881 should be 0\"\n",
        "\n",
        "# total fat check for id 112140\n",
        "assert round(raw_recipes_df.filter(\"id == 112140\").select('total_fat_per_100_cal').first()[0]) == 8, \"total_fat_per_100_cal for recipe 112140 should be 8\"\n",
        "\n",
        "# checking for nulls\n",
        "for c in ['total_fat_per_100_cal','sugar_per_100_cal','sodium_per_100_cal','protein_per_100_cal',\n",
        "                          'saturated_fat_per_100_cal','carbohydrates_per_100_cal']:\n",
        "    assert raw_recipes_df.select(F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c)).collect()[0][0] == 0, \"There are Nulls in the data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "id": "bef4a786",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bef4a786",
        "outputId": "b7a8d781-8095-476c-8e75-7c84631ba081"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ],
      "source": [
        "raw_recipes_df.filter(\"id == 28881\").select('total_fat_per_100_cal').first()[0] == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "id": "7e16a770",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e16a770",
        "outputId": "c700b58b-44e5-4a69-fb58-d67760457c6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ],
      "source": [
        "round(raw_recipes_df.filter(\"id == 112140\").select('total_fat_per_100_cal').first()[0]) == 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "id": "26c038a1",
      "metadata": {
        "id": "26c038a1"
      },
      "outputs": [],
      "source": [
        "for c in ['total_fat_per_100_cal','sugar_per_100_cal','sodium_per_100_cal','protein_per_100_cal',\n",
        "                          'saturated_fat_per_100_cal','carbohydrates_per_100_cal']:\n",
        "    assert raw_recipes_df.select(F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c)).collect()[0][0] == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "id": "c9fdb7c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9fdb7c0",
        "outputId": "6b4dc184-1e17-4e7d-ebf9-18085f306028"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- id: integer (nullable = true)\n",
            " |-- minutes: integer (nullable = true)\n",
            " |-- contributor_id: integer (nullable = true)\n",
            " |-- submitted: date (nullable = true)\n",
            " |-- tags: string (nullable = true)\n",
            " |-- nutrition: string (nullable = true)\n",
            " |-- n_steps: integer (nullable = true)\n",
            " |-- steps: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- ingredients: string (nullable = true)\n",
            " |-- n_ingredients: integer (nullable = true)\n",
            " |-- calories: float (nullable = true)\n",
            " |-- total_fat_PDV: float (nullable = true)\n",
            " |-- sugar_PDV: float (nullable = true)\n",
            " |-- sodium_PDV: float (nullable = true)\n",
            " |-- protein_PDV: float (nullable = true)\n",
            " |-- saturated_fat_PDV: float (nullable = true)\n",
            " |-- carbohydrates_PDV: float (nullable = true)\n",
            " |-- total_fat_per_100_cal: double (nullable = false)\n",
            " |-- sugar_per_100_cal: double (nullable = false)\n",
            " |-- sodium_per_100_cal: double (nullable = false)\n",
            " |-- protein_per_100_cal: double (nullable = false)\n",
            " |-- saturated_fat_per_100_cal: double (nullable = false)\n",
            " |-- carbohydrates_per_100_cal: double (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "raw_recipes_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77b2b456",
      "metadata": {
        "id": "77b2b456"
      },
      "source": [
        " #### <font color='red'>If all test cases pass task 03 ends </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bd6ac1d",
      "metadata": {
        "id": "2bd6ac1d"
      },
      "source": [
        "## <font color='red'>Task 04: Convert the tags column from a string to an array of strings </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41905c76",
      "metadata": {
        "id": "41905c76"
      },
      "source": [
        "<font color='red'>\n",
        "    \n",
        "Currently, the tags column is a string column but holds an array of strings.\n",
        "\n",
        "Your task is to convert the tags columns from a string to an array of strings.\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08470494",
      "metadata": {
        "id": "08470494"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "Remove ```[``` ```]``` ```'``` punctuation marks from the tags column.\n",
        "Split the tags column based on the comma delimiter.\n",
        "    \n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b724846",
      "metadata": {
        "id": "4b724846"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "**Sample input**\n",
        "    \n",
        "Tags column in string datatype.\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61c53092",
      "metadata": {
        "id": "61c53092"
      },
      "source": [
        "![task4.01.png](attachment:task4.01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8d04cf2",
      "metadata": {
        "id": "d8d04cf2"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "**Sample outout**\n",
        "    \n",
        "Tags column in array of ArrayType(StringType())  \n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22614b82",
      "metadata": {
        "id": "22614b82"
      },
      "source": [
        "![task4.02.png](attachment:task4.02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7fe1ba7",
      "metadata": {
        "id": "e7fe1ba7"
      },
      "source": [
        "<font color='red'> We have included some test cases given below. You can use them to check if you have completed the task correctly.  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7580844",
      "metadata": {
        "id": "a7580844"
      },
      "source": [
        "### <font color='blue'>Solution to Task 4 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98593e1f",
      "metadata": {
        "id": "98593e1f"
      },
      "source": [
        "<font color='blue'>Complete the code in the following cell</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "id": "6ac8351c",
      "metadata": {
        "id": "6ac8351c"
      },
      "outputs": [],
      "source": [
        "# Task 04 Cell 1 out of 1\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark.sql.functions import split, col\n",
        "raw_recipes_df = (raw_recipes_df\n",
        "                  .withColumn('tags', F.regexp_replace(\"tags\",\"[\\\\[\\\\]\\\\']\",\"\")\n",
        "                             )\n",
        "                  .withColumn('tags', F.split(\"tags\",\", \")\n",
        "                             )\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "id": "8b5f5b6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b5f5b6c",
        "outputId": "49f053f3-cf3f-4bed-a47d-c5b879ca497c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- id: integer (nullable = true)\n",
            " |-- minutes: integer (nullable = true)\n",
            " |-- contributor_id: integer (nullable = true)\n",
            " |-- submitted: date (nullable = true)\n",
            " |-- tags: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            " |-- nutrition: string (nullable = true)\n",
            " |-- n_steps: integer (nullable = true)\n",
            " |-- steps: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- ingredients: string (nullable = true)\n",
            " |-- n_ingredients: integer (nullable = true)\n",
            " |-- calories: float (nullable = true)\n",
            " |-- total_fat_PDV: float (nullable = true)\n",
            " |-- sugar_PDV: float (nullable = true)\n",
            " |-- sodium_PDV: float (nullable = true)\n",
            " |-- protein_PDV: float (nullable = true)\n",
            " |-- saturated_fat_PDV: float (nullable = true)\n",
            " |-- carbohydrates_PDV: float (nullable = true)\n",
            " |-- total_fat_per_100_cal: double (nullable = false)\n",
            " |-- sugar_per_100_cal: double (nullable = false)\n",
            " |-- sodium_per_100_cal: double (nullable = false)\n",
            " |-- protein_per_100_cal: double (nullable = false)\n",
            " |-- saturated_fat_per_100_cal: double (nullable = false)\n",
            " |-- carbohydrates_per_100_cal: double (nullable = false)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "raw_recipes_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abea8362",
      "metadata": {
        "id": "abea8362"
      },
      "source": [
        "**Test cases for Task 04**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "id": "493f28b6",
      "metadata": {
        "id": "493f28b6"
      },
      "outputs": [],
      "source": [
        "assert isinstance(raw_recipes_df.schema[\"tags\"].dataType, ArrayType), \"The 'tags' column is not of ArrayType.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "id": "43c2d075",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43c2d075",
        "outputId": "edbdb8b9-7031-40bd-f0d0-e832ae691312"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 227
        }
      ],
      "source": [
        "raw_recipes_df.schema[\"tags\"].dataType == ArrayType(StringType(), True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "id": "ba721d78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba721d78",
        "outputId": "39f9f719-8ed4-40de-b176-d43585620981"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ],
      "source": [
        "raw_recipes_df.collect()[2][5] == ['time-to-make','course', 'preparation', 'main-dish', 'chili', 'crock-pot-slow-cooker', 'dietary', 'equipment', '4-hours-or-less']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "923e5db1",
      "metadata": {
        "id": "923e5db1"
      },
      "source": [
        "#### <font color='red'>If all test cases pass task 04 ends </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96e3185b",
      "metadata": {
        "id": "96e3185b"
      },
      "source": [
        "## Join Recipe Data to Review Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "id": "63fba4f5",
      "metadata": {
        "id": "63fba4f5"
      },
      "outputs": [],
      "source": [
        "# Reading the second data set.\n",
        "# keep this cell unedited\n",
        "\n",
        "raw_ratings_df = (spark.read.csv(\"RAW_interactions_cleaned.csv\",\n",
        "                                 header=True,\n",
        "                                 inferSchema= True)\n",
        "                  .withColumn(\"review_date\",  F.col(\"date\"))\n",
        "                  .drop(F.col(\"date\"))\n",
        "                  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "id": "31b18096",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31b18096",
        "outputId": "74199d4f-ee41-492a-b156-6133f6e3d762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- recipe_id: integer (nullable = true)\n",
            " |-- rating: integer (nullable = true)\n",
            " |-- review: string (nullable = true)\n",
            " |-- review_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "raw_ratings_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "id": "f4e8cac9",
      "metadata": {
        "id": "f4e8cac9"
      },
      "outputs": [],
      "source": [
        "# Code check cell\n",
        "# Do not edit cells with assert commands\n",
        "# If an error is shown after running this cell, please recheck your code.\n",
        "\n",
        "assert raw_ratings_df.count() == 1132367, \"There is a mistake in reading the data.\"\n",
        "assert len(raw_ratings_df.columns) == 5, \"There is a mistake in reading the data.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "id": "17084afb",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17084afb",
        "outputId": "f440613e-f6b6-4274-9221-b686923565e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+------+--------------------+-----------+\n",
            "|user_id|recipe_id|rating|              review|review_date|\n",
            "+-------+---------+------+--------------------+-----------+\n",
            "|  38094|    40893|     4|Great with a sala...| 2003-02-17|\n",
            "|1293707|    40893|     5|So simple  so del...| 2011-12-21|\n",
            "|   8937|    44394|     4|This worked very ...| 2002-12-01|\n",
            "| 126440|    85009|     5|I made the Mexica...| 2010-02-27|\n",
            "|  57222|    85009|     5|Made the cheddar ...| 2011-10-01|\n",
            "+-------+---------+------+--------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "raw_ratings_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e6335b6",
      "metadata": {
        "id": "9e6335b6"
      },
      "source": [
        "## <font color='red'>Task 05: Read the second data file </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb38527e",
      "metadata": {
        "id": "bb38527e"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "Along with raw recipes data, we also have raw ratings data.\n",
        "\n",
        "The code to read the data is already written above. Your task is to join the raw ratings and raw recipes data.\n",
        "\n",
        "The resulting dataframe must have the same number of rows as in the raw ratings data.\n",
        "    \n",
        "Join both the dataframes using the recipie IDs.\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afe730a0",
      "metadata": {
        "id": "afe730a0"
      },
      "source": [
        "<font color='red'>\n",
        "    \n",
        "**Sample Input**\n",
        "    \n",
        "```raw recipes_df``` and ```raw_ratings_df```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "874180c3",
      "metadata": {
        "id": "874180c3"
      },
      "source": [
        "![task5.01.png](attachment:task5.01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5408d705",
      "metadata": {
        "id": "5408d705"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "**Sample Output**\n",
        "\n",
        "Combined dataframe with 30 columns and 1132367 rows\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f83c600",
      "metadata": {
        "id": "4f83c600"
      },
      "source": [
        "![task5.02.png](attachment:task5.02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7f4fe5a",
      "metadata": {
        "id": "e7f4fe5a"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "**Calculation explanation**\n",
        "\n",
        "There are 25 columns in the ```raw_recipes_df``` and five in the ```raw_ratings_df```. So total columns in the combined dataframe 25 + 5 = 30\n",
        "\n",
        "The number of rows in the combined dataframe must be the same as the rows in the ```raw_ratings_df```. So total rows in combined dataframe 1132367\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7c934d",
      "metadata": {
        "id": "dc7c934d"
      },
      "source": [
        "<font color='red'> We have included some test cases given below. You can use them to check if you have completed the task correctly.  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f87f2f0",
      "metadata": {
        "id": "4f87f2f0"
      },
      "source": [
        "### <font color='blue'>Solution to Task 5 </font>\n",
        "\n",
        "<font color='blue'>Complete the code in the following cell</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "id": "800f2ecc",
      "metadata": {
        "id": "800f2ecc"
      },
      "outputs": [],
      "source": [
        "# Task 05 Cell 1 out of 1\n",
        "\n",
        "interaction_level_df = raw_ratings_df.join(raw_recipes_df,raw_ratings_df.recipe_id==raw_recipes_df.id,\"inner\"\n",
        "                                           # add the key on which the join should happen\n",
        "                                           # mention the type of join expected.\n",
        "                                           )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a20faf3",
      "metadata": {
        "id": "1a20faf3"
      },
      "source": [
        "**Test cases for Task 05**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "id": "7a59d3cb",
      "metadata": {
        "id": "7a59d3cb"
      },
      "outputs": [],
      "source": [
        "# Code check cell\n",
        "# Do not edit cells with assert commands\n",
        "# If an error is shown after running this cell, please recheck your code.\n",
        "\n",
        "assert (interaction_level_df.count() ,len(interaction_level_df.columns)) == (1132367, 30), \"The type of join is incorrect\"\n",
        "\n",
        "list1 = raw_ratings_df.select('recipe_id').collect()\n",
        "list2 = raw_recipes_df.select('id').collect()\n",
        "exclusive_set = set(list1)-set(list2)\n",
        "\n",
        "assert len(exclusive_set) == 0, \"There is a mistake in reading one of the two data files.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "id": "30fe8ee4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30fe8ee4",
        "outputId": "ec23f449-00e2-428c-c2ae-90bfad5c1539"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ],
      "source": [
        "(interaction_level_df.count() ,len(interaction_level_df.columns)) == (1132367, 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "id": "6c3e9dff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c3e9dff",
        "outputId": "4a0c6b9b-c4be-45fd-fede-d77fdcf0d470"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ],
      "source": [
        "len(exclusive_set) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b84f908",
      "metadata": {
        "id": "0b84f908"
      },
      "source": [
        "#### <font color='red'>If all test cases pass task 05 ends </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "470302a4",
      "metadata": {
        "id": "470302a4"
      },
      "source": [
        "## <font color='red'>Task 06:  Create time-based features</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a69cd3",
      "metadata": {
        "id": "41a69cd3"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "Currently, both the date columns, the submitted date, and the review date are in string forms.\n",
        "    \n",
        "First convert the ```submitted``` and ```review_date``` to DateType()\n",
        "\n",
        "Use review date and submission date to derive new features:\n",
        "1. ```days_since_submission_on_review_date``` Number of days between the recipe submission and the current review.  \n",
        "2. ```months_since_submission_on_review_date``` Number of months between the recipe submission and the current review.\n",
        "3. ```years_since_submission_on_review_date```Number of years between the recipe submission and the current review.\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a72e2573",
      "metadata": {
        "id": "a72e2573"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "**Sample input**\n",
        "\n",
        "The following columns need to be used to calculate the time based features.\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "538e9195",
      "metadata": {
        "id": "538e9195"
      },
      "source": [
        "![task6.01.png](attachment:task6.01.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34178367",
      "metadata": {
        "id": "34178367"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "**Sample Output:**\n",
        "\n",
        "New date based features have been added to the interactions dataframe\n",
        "\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82adcc3c",
      "metadata": {
        "id": "82adcc3c"
      },
      "source": [
        "![task6.02.png](attachment:task6.02.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d410639",
      "metadata": {
        "id": "3d410639"
      },
      "source": [
        "<font color='red'>\n",
        "\n",
        "**Sample Calculation**\n",
        "\n",
        "Recipe 40893 was submitted on 2002-09-21\n",
        "User 38094 reviewed recipe 40893 on 2003-02-17\n",
        "\n",
        "```days_since_submission_on_review_date``` number of calender days between 2002-09-21 and 2003-02-17 that is 149\n",
        "\n",
        "```months_since_submission_on_review_date``` number of calender months between 2002-09-21 and 2003-02-17 that is 4.87 (calculated by a pyspark function)\n",
        "\n",
        "```years_since_submission_on_review_date``` number of calender months divided by 12 that is 0.40\n",
        "    \n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc67d3ac",
      "metadata": {
        "id": "dc67d3ac"
      },
      "source": [
        "### <font color='blue'>Solution to Task 6 </font>\n",
        "\n",
        "<font color='blue'>Complete the code in the following cell</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "id": "4325e82b",
      "metadata": {
        "id": "4325e82b"
      },
      "outputs": [],
      "source": [
        "# Task 06 Cell 1 out of 2\n",
        "\n",
        "interaction_level_df = (interaction_level_df\n",
        "                        .withColumn('submitted',F.col(\"submitted\").cast(\"date\") # pyspark function to cast a column to DateType()\n",
        "                                   )\n",
        "                        .withColumn('review_date',F.col(\"review_date\").cast(\"date\") # pyspark function to cast a column to DateType()\n",
        "                                   )\n",
        "\n",
        "                       )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "id": "dc2f5452",
      "metadata": {
        "scrolled": false,
        "id": "dc2f5452"
      },
      "outputs": [],
      "source": [
        "interaction_level_df = (interaction_level_df\n",
        "                        .withColumn('days_since_submission_on_review_date',F.datediff(\"review_date\",\"submitted\")\n",
        "                                     # Pyspark function to find the number of days between two dates\n",
        "                                   )\n",
        "                        .withColumn('months_since_submission_on_review_date',F.months_between(\"review_date\",\"submitted\")\n",
        "                                     # Pyspark function to find the number of months between two dates\n",
        "                                   )\n",
        "                        .withColumn('years_since_submission_on_review_date',F.months_between(\"review_date\",\"submitted\")/12\n",
        "                                     # Pyspark function to find the number of months between two dates / 12\n",
        "                                   )\n",
        "                         )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32301322",
      "metadata": {
        "id": "32301322"
      },
      "source": [
        "**Test cases for Task 06**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "id": "416eb59e",
      "metadata": {
        "id": "416eb59e"
      },
      "outputs": [],
      "source": [
        "# Code check cell\n",
        "# Do not edit cells with assert commands\n",
        "# If an error is shown after running this cell, please recheck your code.\n",
        "\n",
        "assert interaction_level_df.schema[\"days_since_submission_on_review_date\"].dataType == IntegerType()\n",
        "\n",
        "assert (interaction_level_df.filter((interaction_level_df.user_id == 428885) & (interaction_level_df.recipe_id == 335241))\n",
        "                            .select('days_since_submission_on_review_date').collect()[0][0]) == 77\n",
        "assert (interaction_level_df.filter((interaction_level_df.user_id == 2025676) & (interaction_level_df.recipe_id == 94265))\n",
        "                            .select('months_since_submission_on_review_date').collect()[0][0]) == 153.22580645\n",
        "assert (interaction_level_df.filter((interaction_level_df.user_id == 338588) & (interaction_level_df.recipe_id == 21859))\n",
        "                            .select('years_since_submission_on_review_date').collect()[0][0]) == 4.564516129166667"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "id": "afdaf770",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afdaf770",
        "outputId": "42bc028c-64cf-4cae-b31e-978f8343fb3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ],
      "source": [
        "interaction_level_df.schema[\"days_since_submission_on_review_date\"].dataType == IntegerType()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "id": "9da4f93a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9da4f93a",
        "outputId": "df5f4143-3686-46b1-df54-ab7c0dd3371b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ],
      "source": [
        "(interaction_level_df.filter((interaction_level_df.user_id == 428885) & (interaction_level_df.recipe_id == 335241))\n",
        "                            .select('days_since_submission_on_review_date').collect()[0][0]) == 77"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "id": "6baa4356",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6baa4356",
        "outputId": "67caf634-7865-400f-9cd1-f5fa3df3ee3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ],
      "source": [
        "(interaction_level_df.filter((interaction_level_df.user_id == 2025676) & (interaction_level_df.recipe_id == 94265))\n",
        "                            .select('months_since_submission_on_review_date').collect()[0][0]) == 153.22580645"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "id": "e82159a8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e82159a8",
        "outputId": "600cc08e-cf86-4d6b-dbe7-130b57e6ad85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 243
        }
      ],
      "source": [
        "(interaction_level_df.filter((interaction_level_df.user_id == 338588) & (interaction_level_df.recipe_id == 21859))\n",
        "                            .select('years_since_submission_on_review_date').collect()[0][0]) == 4.564516129166667"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a764c83",
      "metadata": {
        "id": "3a764c83"
      },
      "source": [
        "#### <font color='red'>If all test cases pass task 06 ends</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "794e0e3e",
      "metadata": {
        "id": "794e0e3e"
      },
      "source": [
        "## Save the data we have created so far in a parquet file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "id": "24f6017a",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24f6017a",
        "outputId": "c9801694-05bc-4dd7-c959-b187543b97e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- recipe_id: integer (nullable = true)\n",
            " |-- rating: integer (nullable = true)\n",
            " |-- review: string (nullable = true)\n",
            " |-- review_date: date (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- id: integer (nullable = true)\n",
            " |-- minutes: integer (nullable = true)\n",
            " |-- contributor_id: integer (nullable = true)\n",
            " |-- submitted: date (nullable = true)\n",
            " |-- tags: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            " |-- nutrition: string (nullable = true)\n",
            " |-- n_steps: integer (nullable = true)\n",
            " |-- steps: string (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- ingredients: string (nullable = true)\n",
            " |-- n_ingredients: integer (nullable = true)\n",
            " |-- calories: float (nullable = true)\n",
            " |-- total_fat_PDV: float (nullable = true)\n",
            " |-- sugar_PDV: float (nullable = true)\n",
            " |-- sodium_PDV: float (nullable = true)\n",
            " |-- protein_PDV: float (nullable = true)\n",
            " |-- saturated_fat_PDV: float (nullable = true)\n",
            " |-- carbohydrates_PDV: float (nullable = true)\n",
            " |-- total_fat_per_100_cal: double (nullable = false)\n",
            " |-- sugar_per_100_cal: double (nullable = false)\n",
            " |-- sodium_per_100_cal: double (nullable = false)\n",
            " |-- protein_per_100_cal: double (nullable = false)\n",
            " |-- saturated_fat_per_100_cal: double (nullable = false)\n",
            " |-- carbohydrates_per_100_cal: double (nullable = false)\n",
            " |-- days_since_submission_on_review_date: integer (nullable = true)\n",
            " |-- months_since_submission_on_review_date: double (nullable = true)\n",
            " |-- years_since_submission_on_review_date: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "interaction_level_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "id": "864a70a2",
      "metadata": {
        "scrolled": true,
        "id": "864a70a2"
      },
      "outputs": [],
      "source": [
        "assert (interaction_level_df.count() ,len(interaction_level_df.columns) ) == (1132367, 33)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "id": "b4d75376",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4d75376",
        "outputId": "40f43ddc-5def-4159-f857-1dc273fb1dab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ],
      "source": [
        "(interaction_level_df.count() ,len(interaction_level_df.columns) ) == (1132367, 33)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "id": "f66d1048",
      "metadata": {
        "id": "f66d1048"
      },
      "outputs": [],
      "source": [
        "## Write the raw_recipes_df\n",
        "## create a folder named data in you current directry before running this.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "interaction_level_df.write.mode(\"overwrite\").parquet(\"interaction_level_df\") # Modify the path as you need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be51e075",
      "metadata": {
        "id": "be51e075"
      },
      "outputs": [],
      "source": [
        "interaction_level_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0d3d9fe",
      "metadata": {
        "id": "c0d3d9fe"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51a975e4",
      "metadata": {
        "id": "51a975e4"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416e20f4",
      "metadata": {
        "id": "416e20f4"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42be1752",
      "metadata": {
        "id": "42be1752"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbcae33f",
      "metadata": {
        "id": "cbcae33f"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2ef05dc",
      "metadata": {
        "id": "d2ef05dc"
      },
      "outputs": [],
      "source": [
        "# Run this everytime you create a new spark instance.\n",
        "!pip install plotly==5.5.0 pandas==0.25.1 numpy==1.14.5 matplotlib==3.1.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd1b25a1",
      "metadata": {
        "id": "bd1b25a1"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "from IPython import get_ipython\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import for typecasting columns\n",
        "from pyspark.sql.types import IntegerType,BooleanType,DateType,FloatType,StringType\n",
        "from pyspark.sql.types import ArrayType"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3f5076f",
      "metadata": {
        "id": "d3f5076f"
      },
      "source": [
        "## Defining Custom Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da6021fa",
      "metadata": {
        "id": "da6021fa"
      },
      "outputs": [],
      "source": [
        "def get_quantiles(df, col_name, quantiles_list = [0.01, 0.25, 0.5, 0.75, 0.99]):\n",
        "    \"\"\"\n",
        "    Takes a numerical column and returns column values at requested quantiles\n",
        "\n",
        "    Inputs\n",
        "    Argument 1: Dataframe\n",
        "    Argument 2: Name of the column\n",
        "    Argument 3: A list of quantiles you want to find. Default value [0.01, 0.25, 0.5, 0.75, 0.99]\n",
        "\n",
        "    Output\n",
        "    Returns a dictionary with quantiles as keys and column quantile values as values\n",
        "    \"\"\"\n",
        "    # Get min, max and quantile values for given column\n",
        "    min_val = df.agg(F.min(col_name)).first()[0]\n",
        "    max_val = df.agg(F.max(col_name)).first()[0]\n",
        "    quantiles_vals = df.approxQuantile(col_name,\n",
        "                                       quantiles_list,\n",
        "                                       0)\n",
        "\n",
        "    # Store min, quantiles and max in output dict, sequentially\n",
        "    quantiles_dict = {0.0:min_val}\n",
        "    quantiles_dict.update(dict(zip(quantiles_list, quantiles_vals)))\n",
        "    quantiles_dict.update({1.0:max_val})\n",
        "    return(quantiles_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6048acf2",
      "metadata": {
        "id": "6048acf2"
      },
      "outputs": [],
      "source": [
        "def plot_bucketwise_statistics (summary, bucketizer):\n",
        "    \"\"\"\n",
        "    Takes in a dataframe and a bucketizer object and plots the summary statistics for each bucket in the dataframe.\n",
        "\n",
        "    Inputs\n",
        "    Argument 1: Pandas dataframe obtained from bucket_col_print_summary function\n",
        "    Argument 2: Bucketizer object obtained from bucket_col_print_summary function\n",
        "\n",
        "    Output\n",
        "    Displays a plot of bucketwise average ratings nunber of ratings of a parameter.\n",
        "    \"\"\"\n",
        "    # Creating bucket labels from splits\n",
        "    classlist = bucketizer.getSplits()\n",
        "    number_of_classes = len(classlist) - 1\n",
        "\n",
        "    class_labels = []\n",
        "    hover_labels = []\n",
        "    for i in range (number_of_classes):\n",
        "        hover_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) +\" (Bucket name: \"+ str(int(i)) +\")\"  )\n",
        "        class_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) )\n",
        "\n",
        "    summary[\"Scaled_number\"] = (summary[\"n_ratings\"]-summary[\"n_ratings\"].min())/(summary[\"n_ratings\"].max()-summary[\"n_ratings\"].min()) + 1.5\n",
        "    summary['Bucket_Names'] = class_labels\n",
        "\n",
        "    # making plot\n",
        "    x = summary[\"Bucket_Names\"]\n",
        "    y1 = summary[\"avg_rating\"]\n",
        "    y2 = summary[\"n_ratings\"]\n",
        "    err = summary[\"stddev_rating\"]\n",
        "\n",
        "    # Plot scatter here\n",
        "    plt.rcParams[\"figure.figsize\"] = [summary.shape[0]+2, 6.0]\n",
        "    plt.rcParams[\"figure.autolayout\"] = True\n",
        "    fig, ax1 = plt.subplots()\n",
        "\n",
        "    bar = ax1.bar(x, y1, color = \"#262261\")\n",
        "    ax1.errorbar(x, y1, yerr=err, fmt=\"o\", color=\"#EE4036\")\n",
        "    ax1.set(ylim=(0, 7))\n",
        "\n",
        "    #ax1.bar_label(bar , fmt='%.2f', label_type='edge')\n",
        "    def barlabel(x_list,y_list):\n",
        "        for i in range(len(x_list)):\n",
        "            ax1.text(i,y_list[i] + 0.2,y_list[i], ha = 'center',\n",
        "  \t\t\t         fontdict=dict(size=10),\n",
        "  \t\t\t         bbox=dict(facecolor='#262261', alpha=0.2)\n",
        "  \t\t\t        )\n",
        "    barlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"avg_rating\"].round(2).tolist())\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.scatter(x, y2, s=summary[\"Scaled_number\"]*500, c = '#FAAF40')\n",
        "    ax2.set(ylim=(0, summary[\"n_ratings\"].max()*1.15))\n",
        "    def scatterlabel(x_list,y_list):\n",
        "  \t    for i in range(len(x_list)):\n",
        "  \t\t    ax2.text(i,y_list[i] + 15000,y_list[i], ha = 'center',\n",
        "  \t\t\t\t\t fontdict=dict(size=10),\n",
        "                     bbox=dict(facecolor='#FAAF40', alpha=0.5)\n",
        "  \t\t\t\t\t)\n",
        "    scatterlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"n_ratings\"].tolist())\n",
        "\n",
        "    # giving labels to the axises\n",
        "    ax1.set_xlabel(bucketizer.getOutputCol(), fontdict=dict(size=14))\n",
        "    ax1.set_ylabel(\"Average Ratings\",fontdict=dict(size=14))\n",
        "\n",
        "    # secondary y-axis label\n",
        "    ax2.set_ylabel('Number of Ratings',fontdict=dict(size=14))\n",
        "\n",
        "    #plot Title\n",
        "    plt.title('Bucketwise average ratings and number of ratings for \\n'+bucketizer.getInputCol(),\n",
        "              fontdict=dict(size=14))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448c448d",
      "metadata": {
        "id": "448c448d"
      },
      "outputs": [],
      "source": [
        "def bucket_col_print_summary(df, splits, inputCol, outputCol):\n",
        "    \"\"\"\n",
        "    Given a numerical column in a data frame, adds a bucketized version of the column to the data frame, according to splits provided.\n",
        "    Also prints a summary of ratings seen in each bucket made.\n",
        "\n",
        "    Inputs\n",
        "    Argument 1: Data Frame\n",
        "    Argument 2: Values at which the column will be split\n",
        "    Argument 3: Name of the input column (numerical column)\n",
        "    Argument 4: Name of the output column (bucketized numerical column)\n",
        "\n",
        "    Output:\n",
        "    1) New dataframe with the output column added\n",
        "    2) Bucketizer object trained from the input column\n",
        "    3) Pandas dataframe with summary statistics for ratings seen in buckets of the output column\n",
        "    Also plots summary statistics for ratings seen in buckets of the output column\n",
        "    \"\"\"\n",
        "\n",
        "    # Dropping bucket if it already exists\n",
        "    if outputCol in df.columns:\n",
        "        df = df.drop(outputCol)\n",
        "\n",
        "    # Training bucketizer\n",
        "    bucketizer = Bucketizer(splits = splits,\n",
        "                            inputCol  = inputCol,\n",
        "                            outputCol = outputCol)\n",
        "\n",
        "    df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
        "\n",
        "    # Printing meta information on buckets created\n",
        "    print(\"Added bucketized column {}\".format(outputCol))\n",
        "    print(\"\")\n",
        "    print(\"Bucketing done for split definition: {}\".format(splits))\n",
        "    print(\"\")\n",
        "    print(\"Printing summary statistics for ratings in buckets below:\")\n",
        "\n",
        "    # Creating a summary statistics dataframe and passing it to the plotting function\n",
        "    summary =  (df\n",
        "                .groupBy(outputCol)\n",
        "                .agg(F.avg('rating').alias('avg_rating'),\n",
        "                     F.stddev('rating').alias('stddev_rating'),\n",
        "                     F.count('rating').alias('n_ratings'))\n",
        "                .sort(outputCol)\n",
        "                .toPandas())\n",
        "\n",
        "    plot_bucketwise_statistics(summary,bucketizer)\n",
        "\n",
        "    return df, bucketizer, summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "756b8038",
      "metadata": {
        "id": "756b8038"
      },
      "outputs": [],
      "source": [
        "def get_column_distribution_summary(df, col_name):\n",
        "    \"\"\"\n",
        "    Takes a column in a data frame and prints the summary statistics (average, standard deviation, count and distinct count) for all unique values in that column.\n",
        "\n",
        "    Inputs\n",
        "    Argument 1: Dataframe\n",
        "    Argument 2: Name of the column\n",
        "\n",
        "    Output\n",
        "    Returns nothing\n",
        "    Prints a Dataframe with summary statistics\n",
        "    \"\"\"\n",
        "    print(df\n",
        "          .groupBy(col_name)\n",
        "          .agg(F.avg('rating').alias('avg_rating'),\n",
        "               F.stddev('rating').alias('stddev_rating'),\n",
        "               F.count('rating').alias('n_ratings'),\n",
        "               F.countDistinct('id').alias('n_recipes'))\n",
        "          .sort(F.col(col_name).asc())\n",
        "          .show(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d25093",
      "metadata": {
        "id": "69d25093"
      },
      "outputs": [],
      "source": [
        "def get_n_items_satisfying_condition (df, condition, aggregation_level = \"recipe\"):\n",
        "    \"\"\"\n",
        "    Given a condition, find the number of recipes / reviews that match the condition.\n",
        "    Also calculates the percentage of such recipes / reviews as a percentage of all recipes / reviews.\n",
        "\n",
        "    Inputs\n",
        "    Argument 1: Dataframe\n",
        "    Argument 2: Logical expression describing a condition, string type. eg: \"minutes == 0\"\n",
        "    Argument 3: Aggregation level for determining \"items\", either  \"recipe\" or \"review\". Default value == \"recipe\"\n",
        "\n",
        "    Output: Returns no object.\n",
        "    Prints the following:\n",
        "    1) Number of recipes / reviews that satisfy the condition\n",
        "    2) Total number of recipes / reviews in the dataframe\n",
        "    3) Percentage of recipes / reviews that satisfy the condition\n",
        "    \"\"\"\n",
        "    # Find out num rows satisfying the condition\n",
        "    if aggregation_level == \"recipe\":\n",
        "        number_of_rows_satisfying_condition = (df\n",
        "                                             .filter(condition)\n",
        "                                             .agg(F.countDistinct(\"id\"))).first()[0]\n",
        "\n",
        "        n_rows_total = (df.agg(F.countDistinct(\"id\"))).first()[0]\n",
        "    if aggregation_level == \"review\":\n",
        "        number_of_rows_satisfying_condition = (df\n",
        "                                             .filter(condition)\n",
        "                                             .agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
        "        n_rows_total = (df.agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
        "\n",
        "    # Find out % rows satisfying the conditon and print a properly formatted output\n",
        "    perc_rows = round(number_of_rows_satisfying_condition * 100/ n_rows_total, 2)\n",
        "    print('Condition String                   : \"{}\"'.format(condition))\n",
        "    print(\"Num {}s Satisfying Condition   : {} [{}%]\".format(aggregation_level.title(), number_of_rows_satisfying_condition, perc_rows))\n",
        "    print(\"Total Num {}s                  : {}\".format(aggregation_level.title(), n_rows_total))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7abb148",
      "metadata": {
        "id": "b7abb148"
      },
      "source": [
        "## Read the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8407eaa7",
      "metadata": {
        "id": "8407eaa7"
      },
      "source": [
        "- Read ```interaction_level_df_processed```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e101d5d",
      "metadata": {
        "id": "7e101d5d"
      },
      "outputs": [],
      "source": [
        "interaction_level_df_processed = spark.read.parquet(\"interaction_level_df\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a130b924",
      "metadata": {
        "id": "a130b924"
      },
      "outputs": [],
      "source": [
        "# Code check cell\n",
        "# Do not edit cells with assert commands\n",
        "# If an error is shown after running this cell, please recheck your code.\n",
        "\n",
        "assert interaction_level_df.count() == 1132367, \"There is a mistake in reading the data.\"\n",
        "assert len(interaction_level_df.columns) == 33, \"There is a mistake in reading the data.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9d38042",
      "metadata": {
        "id": "b9d38042"
      },
      "outputs": [],
      "source": [
        "interaction_level_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f59f91f3",
      "metadata": {
        "id": "f59f91f3"
      },
      "source": [
        "## Bucketing and Cleaning Numerical Features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24dd5a51",
      "metadata": {
        "id": "24dd5a51"
      },
      "source": [
        "#### **1. `years_since_submission_on_review_date`**\n",
        "[Review Time Since Submission]\n",
        "- Recipes more than 6 years old are rated low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1f7675",
      "metadata": {
        "id": "9b1f7675"
      },
      "outputs": [],
      "source": [
        "get_quantiles(df = interaction_level_df,\n",
        "             col_name = \"years_since_submission_on_review_date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5dbd2b",
      "metadata": {
        "id": "7d5dbd2b"
      },
      "outputs": [],
      "source": [
        "get_n_items_satisfying_condition(df = interaction_level_df,\n",
        "                                 condition= 'years_since_submission_on_review_date < 0',\n",
        "                                 aggregation_level= \"review\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1d1bc35",
      "metadata": {
        "id": "f1d1bc35"
      },
      "outputs": [],
      "source": [
        "# Only keep interactions with review dates >= recipe submission date\n",
        "\n",
        "interaction_level_df = (interaction_level_df\n",
        "                        .filter('years_since_submission_on_review_date >= 0'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b4a910",
      "metadata": {
        "id": "70b4a910"
      },
      "outputs": [],
      "source": [
        "splits = [ 0, 1, 3, 6, float('Inf')]\n",
        "inputCol  = \"years_since_submission_on_review_date\"\n",
        "outputCol = \"years_since_submission_on_review_date_bucket\"\n",
        "\n",
        "(interaction_level_df, submission_time_bucketizer, submission_time_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
        "                                                                              splits = splits,\n",
        "                                                                              inputCol  = inputCol,\n",
        "                                                                              outputCol = outputCol)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39629802",
      "metadata": {
        "id": "39629802"
      },
      "source": [
        "#### **2. `minutes`**\n",
        "\n",
        "[prep time]\n",
        "- Somewhat relevant\n",
        "- Low prep time is preferred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d76b23",
      "metadata": {
        "id": "67d76b23"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "from IPython import get_ipython\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import for typecasting columns\n",
        "from pyspark.sql.types import IntegerType,BooleanType,DateType,FloatType,StringType\n",
        "from pyspark.sql.types import ArrayType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8da9276c",
      "metadata": {
        "id": "8da9276c"
      },
      "outputs": [],
      "source": [
        "get_quantiles(df = interaction_level_df,\n",
        "              col_name = \"minutes\",\n",
        "              quantiles_list=[0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d3699b8",
      "metadata": {
        "id": "3d3699b8"
      },
      "outputs": [],
      "source": [
        "# Capping prep time at 930 minutes\n",
        "\n",
        "interaction_level_df = (interaction_level_df\n",
        "                        .withColumn(\"minutes\",\n",
        "                                    F.when(interaction_level_df[\"minutes\"] > 930, 930)\n",
        "                                     .otherwise(interaction_level_df[\"minutes\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b790d798",
      "metadata": {
        "id": "b790d798"
      },
      "outputs": [],
      "source": [
        "# investigating recipes with minutes = 0 -> Look at n_steps for such recipes.\n",
        "\n",
        "get_column_distribution_summary(df = (interaction_level_df\n",
        "                                      .filter('minutes == 0')\n",
        "                                      .withColumn('n_steps_modified', (F.when(interaction_level_df['n_steps'] >= 10, \">= 10\")\n",
        "                                                                        .otherwise(F.lpad(interaction_level_df['n_steps'],2,\"0\"))))),\n",
        "                                col_name = 'n_steps_modified')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea44b014",
      "metadata": {
        "id": "ea44b014"
      },
      "outputs": [],
      "source": [
        "# let's look at some examples with 1 step only to see if this makes sense\n",
        "\n",
        "interaction_level_df.filter('minutes == 0 and n_steps == 1').show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "522fcf7f",
      "metadata": {
        "id": "522fcf7f"
      },
      "outputs": [],
      "source": [
        "get_n_items_satisfying_condition(df = interaction_level_df,\n",
        "                                 condition = 'minutes == 0',\n",
        "                                 aggregation_level = \"recipe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ff4c8fc",
      "metadata": {
        "id": "3ff4c8fc"
      },
      "outputs": [],
      "source": [
        "# Remove recipes with cook time zero\n",
        "\n",
        "interaction_level_df = interaction_level_df.filter(\"minutes > 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c3be708",
      "metadata": {
        "id": "9c3be708"
      },
      "outputs": [],
      "source": [
        "splits = [0, 5, 15, 30, 60, 300, 900, float('Inf')]\n",
        "inputCol  = \"minutes\"\n",
        "outputCol = \"prep_time_bucket\"\n",
        "\n",
        "(interaction_level_df, prep_time_bucketizer, prep_time_summary_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
        "                                                                              splits = splits,\n",
        "                                                                              inputCol  = inputCol,\n",
        "                                                                              outputCol = outputCol)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "820277a9",
      "metadata": {
        "id": "820277a9"
      },
      "source": [
        "**3. `n_steps`**\n",
        "\n",
        "- Clearly relevant\n",
        "- Recipes with less than 2 steps are rated high\n",
        "- Recipes with more than 29 steps are rated very low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daa78cb8",
      "metadata": {
        "id": "daa78cb8"
      },
      "outputs": [],
      "source": [
        "get_quantiles(df = interaction_level_df,\n",
        "              col_name = \"n_steps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2a237b0",
      "metadata": {
        "id": "f2a237b0"
      },
      "outputs": [],
      "source": [
        "interaction_level_df.filter('n_steps == 0').show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9c35b9a",
      "metadata": {
        "id": "b9c35b9a"
      },
      "outputs": [],
      "source": [
        "get_n_items_satisfying_condition(df = interaction_level_df,\n",
        "                                 condition = 'n_steps == 0',\n",
        "                                 aggregation_level = \"recipe\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e59edc",
      "metadata": {
        "id": "e5e59edc"
      },
      "outputs": [],
      "source": [
        "# Remove recipes with n_steps zero\n",
        "\n",
        "interaction_level_df = interaction_level_df.filter(\"n_steps > 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0b43fd1",
      "metadata": {
        "id": "b0b43fd1"
      },
      "outputs": [],
      "source": [
        "splits = [0, 2, 6, 8, 12, 29, float('Inf')]\n",
        "inputCol  = \"n_steps\"\n",
        "outputCol = \"n_steps_bucket\"\n",
        "\n",
        "(interaction_level_df, n_steps_bucketizer, n_steps_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
        "                                                                              splits = splits,\n",
        "                                                                              inputCol  = inputCol,\n",
        "                                                                              outputCol = outputCol)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcd28c63",
      "metadata": {
        "id": "dcd28c63"
      },
      "source": [
        "**4. `n_ingredients`**\n",
        "- Not relevant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d41b874",
      "metadata": {
        "id": "3d41b874"
      },
      "outputs": [],
      "source": [
        "get_quantiles(df = interaction_level_df,\n",
        "              col_name = \"n_ingredients\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c797bc9",
      "metadata": {
        "id": "8c797bc9"
      },
      "outputs": [],
      "source": [
        "splits = [0, 6, 9, 11, float('Inf')]\n",
        "inputCol  = \"n_ingredients\"\n",
        "outputCol = \"n_ingredients_bucket\"\n",
        "\n",
        "(interaction_level_df, n_ingredients_bucketizer, n_ingredients_pandas_df) = bucket_col_print_summary(df = interaction_level_df,\n",
        "                                                                              splits = splits,\n",
        "                                                                              inputCol  = inputCol,\n",
        "                                                                              outputCol = outputCol)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2990c1f",
      "metadata": {
        "id": "a2990c1f"
      },
      "source": [
        "**5. `nutrition` columns**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9165fab0",
      "metadata": {
        "id": "9165fab0"
      },
      "source": [
        "- `calories` - Calories per serving seems irrelevant\n",
        "- `fat (per 100 cal)` - Calories per serving seems irrelevant\n",
        "- `sugar (per 100 cal)` - Calories per serving seems irrelevant\n",
        "- `sodium (per 100 cal)` - Calories per serving seems irrelevant\n",
        "- `protein (per 100 cal)` - Calories per serving seems irrelevant\n",
        "- `sat. fat (per 100 cal)` - Calories per serving seems irrelevant\n",
        "- `carbs (per 100 cal)` - Calories per serving seems irrelevant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e847a5e",
      "metadata": {
        "id": "6e847a5e"
      },
      "outputs": [],
      "source": [
        "interaction_level_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f2a918",
      "metadata": {
        "id": "f0f2a918"
      },
      "outputs": [],
      "source": [
        "nutrition_cols = ['calories',\n",
        "                  'total_fat_PDV',\n",
        "                  'sugar_PDV',\n",
        "                  'sodium_PDV',\n",
        "                  'protein_PDV',\n",
        "                  'saturated_fat_PDV',\n",
        "                  'carbohydrates_PDV',\n",
        "                  'total_fat_per_100_cal',\n",
        "                  'sugar_per_100_cal',\n",
        "                  'sodium_per_100_cal',\n",
        "                  'protein_per_100_cal',\n",
        "                  'saturated_fat_per_100_cal',\n",
        "                  'carbohydrates_per_100_cal']\n",
        "\n",
        "quantiles_list = [0.00, 0.05, 0.25, 0.5, 0.75, 0.95, 1.00]\n",
        "nutrition_col_quantiles = pd.DataFrame(index = quantiles_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3c5e3e",
      "metadata": {
        "id": "ab3c5e3e"
      },
      "outputs": [],
      "source": [
        "for col in nutrition_cols:\n",
        "    nutrition_col_quantiles[col] = (get_quantiles(df = interaction_level_df,\n",
        "                                                col_name = col,\n",
        "                                                quantiles_list=quantiles_list)\n",
        "                                  .values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c89659",
      "metadata": {
        "id": "49c89659"
      },
      "outputs": [],
      "source": [
        "nutrition_col_quantile_summary = pd.DataFrame(index = [\"0.00-0.25\", \"0.25-0.50\", \"0.50-0.75\", \"0.75-0.95\", \"0.95 - 1.00\"])\n",
        "\n",
        "for col in nutrition_cols:\n",
        "    splits = ([0]\n",
        "            + list(nutrition_col_quantiles.loc[[0.25, 0.5, 0.75, 0.95], col].round())\n",
        "            + [float('Inf')])\n",
        "    inputCol  = col\n",
        "    outputCol = col+\"_bucket\"\n",
        "\n",
        "    if outputCol in interaction_level_df.columns:\n",
        "        interaction_level_df = interaction_level_df.drop(outputCol)\n",
        "\n",
        "  # Training bucketizer\n",
        "    bucketizer = Bucketizer(splits = splits,\n",
        "                          inputCol  = inputCol,\n",
        "                          outputCol = outputCol)\n",
        "\n",
        "    interaction_level_df = bucketizer.setHandleInvalid(\"keep\").transform(interaction_level_df)\n",
        "\n",
        "    nutrition_col_quantile_summary.loc[:, col] = (interaction_level_df\n",
        "                                                .groupBy(outputCol)\n",
        "                                                .agg(F.avg('rating').alias('avg_rating'))\n",
        "                                                .sort(outputCol)\n",
        "                                                .select('avg_rating').toPandas().values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "143655d8",
      "metadata": {
        "id": "143655d8"
      },
      "outputs": [],
      "source": [
        "# set the max columns to none\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12ac706",
      "metadata": {
        "id": "b12ac706"
      },
      "outputs": [],
      "source": [
        "nutrition_col_quantile_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8665c05b",
      "metadata": {
        "id": "8665c05b"
      },
      "outputs": [],
      "source": [
        "## Writing the modified data to S3\n",
        "interaction_level_df.write.parquet(\"interaction_level_df_processed_data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02efc03d",
      "metadata": {
        "id": "02efc03d"
      },
      "source": [
        "### Feature Extraction Part-2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccded233",
      "metadata": {
        "id": "ccded233"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9298dc4b",
      "metadata": {
        "id": "9298dc4b"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e191fb7d",
      "metadata": {
        "id": "e191fb7d"
      },
      "outputs": [],
      "source": [
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b35e571",
      "metadata": {
        "id": "5b35e571"
      },
      "outputs": [],
      "source": [
        "# Install the required packages\n",
        "!pip install plotly==5.5.0 pandas==0.25.1 numpy==1.14.5 matplotlib==3.1.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d685fd2",
      "metadata": {
        "id": "2d685fd2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml.feature import Bucketizer\n",
        "from pyspark.sql.window import Window\n",
        "from IPython import get_ipython\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Import for typecasting columns\n",
        "from pyspark.sql.types import IntegerType,BooleanType,DateType,FloatType,StringType\n",
        "from pyspark.sql.types import ArrayType"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181be012",
      "metadata": {
        "id": "181be012"
      },
      "source": [
        "## Defining Custom Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f1ff71",
      "metadata": {
        "id": "29f1ff71"
      },
      "outputs": [],
      "source": [
        "def get_quantiles(df, col_name, quantiles_list = [0.01, 0.25, 0.5, 0.75, 0.99]):\n",
        "    \"\"\"\n",
        "    Takes a numerical column and returns column values at requested quantiles\n",
        "\n",
        "    Inputs\n",
        "    Argument 1: Dataframe\n",
        "    Argument 2: Name of the column\n",
        "    Argument 3: A list of quantiles you want to find. Default value [0.01, 0.25, 0.5, 0.75, 0.99]\n",
        "\n",
        "    Output\n",
        "    Returns a dictionary with quantiles as keys and column quantile values as values\n",
        "    \"\"\"\n",
        "    # Get min, max and quantile values for given column\n",
        "    min_val = df.agg(F.min(col_name)).first()[0]\n",
        "    max_val = df.agg(F.max(col_name)).first()[0]\n",
        "    quantiles_vals = df.approxQuantile(col_name,\n",
        "                                       quantiles_list,\n",
        "                                       0)\n",
        "\n",
        "    # Store min, quantiles and max in output dict, sequentially\n",
        "    quantiles_dict = {0.0:min_val}\n",
        "    quantiles_dict.update(dict(zip(quantiles_list, quantiles_vals)))\n",
        "    quantiles_dict.update({1.0:max_val})\n",
        "    return(quantiles_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b01fc79",
      "metadata": {
        "id": "8b01fc79"
      },
      "outputs": [],
      "source": [
        "def plot_bucketwise_statistics (summary, bucketizer):\n",
        "    \"\"\"\n",
        "    Takes in a dataframe and a bucketizer object and plots the summary statistics for each bucket in the dataframe.\n",
        "\n",
        "    Inputs\n",
        "    Argument 1: Pandas dataframe obtained from bucket_col_print_summary function\n",
        "    Argument 2: Bucketizer object obtained from bucket_col_print_summary function\n",
        "\n",
        "    Output\n",
        "    Displays a plot of bucketwise average ratings nunber of ratings of a parameter.\n",
        "    \"\"\"\n",
        "    # Creating bucket labels from splits\n",
        "    classlist = bucketizer.getSplits()\n",
        "    number_of_classes = len(classlist) - 1\n",
        "\n",
        "    class_labels = []\n",
        "    hover_labels = []\n",
        "    for i in range (number_of_classes):\n",
        "        hover_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) +\" (Bucket name: \"+ str(int(i)) +\")\"  )\n",
        "        class_labels.append(str(classlist[i])+\"-\"+str(classlist[i+1]) )\n",
        "\n",
        "    summary[\"Scaled_number\"] = (summary[\"n_ratings\"]-summary[\"n_ratings\"].min())/(summary[\"n_ratings\"].max()-summary[\"n_ratings\"].min()) + 1.5\n",
        "    summary['Bucket_Names'] = class_labels\n",
        "\n",
        "    # making plot\n",
        "    x = summary[\"Bucket_Names\"]\n",
        "    y1 = summary[\"avg_rating\"]\n",
        "    y2 = summary[\"n_ratings\"]\n",
        "    err = summary[\"stddev_rating\"]\n",
        "\n",
        "    # Plot scatter here\n",
        "    plt.rcParams[\"figure.figsize\"] = [summary.shape[0]+2, 6.0]\n",
        "    plt.rcParams[\"figure.autolayout\"] = True\n",
        "    fig, ax1 = plt.subplots()\n",
        "\n",
        "    bar = ax1.bar(x, y1, color = \"#262261\")\n",
        "    ax1.errorbar(x, y1, yerr=err, fmt=\"o\", color=\"#EE4036\")\n",
        "    ax1.set(ylim=(0, 7))\n",
        "\n",
        "    #ax1.bar_label(bar , fmt='%.2f', label_type='edge')\n",
        "    def barlabel(x_list,y_list):\n",
        "        for i in range(len(x_list)):\n",
        "            ax1.text(i,y_list[i] + 0.2,y_list[i], ha = 'center',\n",
        "  \t\t\t         fontdict=dict(size=10),\n",
        "  \t\t\t         bbox=dict(facecolor='#262261', alpha=0.2)\n",
        "  \t\t\t        )\n",
        "    barlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"avg_rating\"].round(2).tolist())\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.scatter(x, y2, s=summary[\"Scaled_number\"]*500, c = '#FAAF40')\n",
        "    ax2.set(ylim=(0, summary[\"n_ratings\"].max()*1.15))\n",
        "    def scatterlabel(x_list,y_list):\n",
        "  \t    for i in range(len(x_list)):\n",
        "  \t\t    ax2.text(i,y_list[i] + 15000,y_list[i], ha = 'center',\n",
        "  \t\t\t\t\t fontdict=dict(size=10),\n",
        "                     bbox=dict(facecolor='#FAAF40', alpha=0.5)\n",
        "  \t\t\t\t\t)\n",
        "    scatterlabel(summary[\"Bucket_Names\"].tolist() ,summary[\"n_ratings\"].tolist())\n",
        "\n",
        "    # giving labels to the axises\n",
        "    ax1.set_xlabel(bucketizer.getOutputCol(), fontdict=dict(size=14))\n",
        "    ax1.set_ylabel(\"Average Ratings\",fontdict=dict(size=14))\n",
        "\n",
        "    # secondary y-axis label\n",
        "    ax2.set_ylabel('Number of Ratings',fontdict=dict(size=14))\n",
        "\n",
        "    #plot Title\n",
        "    plt.title('Bucketwise average ratings and number of ratings for \\n'+bucketizer.getInputCol(),\n",
        "              fontdict=dict(size=14))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dabd48c",
      "metadata": {
        "id": "0dabd48c"
      },
      "outputs": [],
      "source": [
        "def bucket_col_print_summary(df, splits, inputCol, outputCol):\n",
        "    \"\"\"\n",
        "    Given a numerical column in a data frame, adds a bucketized version of the column to the data frame, according to splits provided.\n",
        "    Also prints a summary of ratings seen in each bucket made.\n",
        "\n",
        "    Inputs\n",
        "    Argument 1: Data Frame\n",
        "    Argument 2: Values at which the column will be split\n",
        "    Argument 3: Name of the input column (numerical column)\n",
        "    Argument 4: Name of the output column (bucketized numerical column)\n",
        "\n",
        "    Output:\n",
        "    1) New dataframe with the output column added\n",
        "    2) Bucketizer object trained from the input column\n",
        "    3) Pandas dataframe with summary statistics for ratings seen in buckets of the output column\n",
        "    Also plots summary statistics for ratings seen in buckets of the output column\n",
        "    \"\"\"\n",
        "\n",
        "    # Dropping bucket if it already exists\n",
        "    if outputCol in df.columns:\n",
        "        df = df.drop(outputCol)\n",
        "\n",
        "    # Training bucketizer\n",
        "    bucketizer = Bucketizer(splits = splits,\n",
        "                            inputCol  = inputCol,\n",
        "                            outputCol = outputCol)\n",
        "\n",
        "    df = bucketizer.setHandleInvalid(\"keep\").transform(df)\n",
        "\n",
        "    # Printing meta information on buckets created\n",
        "    print(\"Added bucketized column {}\".format(outputCol))\n",
        "    print(\"\")\n",
        "    print(\"Bucketing done for split definition: {}\".format(splits))\n",
        "    print(\"\")\n",
        "    print(\"Printing summary statistics for ratings in buckets below:\")\n",
        "\n",
        "    # Creating a summary statistics dataframe and passing it to the plotting function\n",
        "    summary =  (df\n",
        "                .groupBy(outputCol)\n",
        "                .agg(F.avg('rating').alias('avg_rating'),\n",
        "                     F.stddev('rating').alias('stddev_rating'),\n",
        "                     F.count('rating').alias('n_ratings'))\n",
        "                .sort(outputCol)\n",
        "                .toPandas())\n",
        "\n",
        "    plot_bucketwise_statistics(summary,bucketizer)\n",
        "\n",
        "    return df, bucketizer, summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531d65b4",
      "metadata": {
        "id": "531d65b4"
      },
      "outputs": [],
      "source": [
        "def get_column_distribution_summary(df, col_name):\n",
        "    \"\"\"\n",
        "    Takes a column in a data frame and prints the summary statistics (average, standard deviation, count and distinct count) for all unique values in that column.\n",
        "\n",
        "    Inputs\n",
        "    Argument 1: Dataframe\n",
        "    Argument 2: Name of the column\n",
        "\n",
        "    Output\n",
        "    Returns nothing\n",
        "    Prints a Dataframe with summary statistics\n",
        "    \"\"\"\n",
        "    print(df\n",
        "          .groupBy(col_name)\n",
        "          .agg(F.avg('rating').alias('avg_rating'),\n",
        "               F.stddev('rating').alias('stddev_rating'),\n",
        "               F.count('rating').alias('n_ratings'),\n",
        "               F.countDistinct('id').alias('n_recipes'))\n",
        "          .sort(F.col(col_name).asc())\n",
        "          .show(50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec0b6af",
      "metadata": {
        "id": "7ec0b6af"
      },
      "outputs": [],
      "source": [
        "def get_n_items_satisfying_condition (df, condition, aggregation_level = \"recipe\"):\n",
        "    \"\"\"\n",
        "    Given a condition, find the number of recipes / reviews that match the condition.\n",
        "    Also calculates the percentage of such recipes / reviews as a percentage of all recipes / reviews.\n",
        "\n",
        "    Inputs\n",
        "    Argument 1: Dataframe\n",
        "    Argument 2: Logical expression describing a condition, string type. eg: \"minutes == 0\"\n",
        "    Argument 3: Aggregation level for determining \"items\", either  \"recipe\" or \"review\". Default value == \"recipe\"\n",
        "\n",
        "    Output: Returns no object.\n",
        "    Prints the following:\n",
        "    1) Number of recipes / reviews that satisfy the condition\n",
        "    2) Total number of recipes / reviews in the dataframe\n",
        "    3) Percentage of recipes / reviews that satisfy the condition\n",
        "    \"\"\"\n",
        "    # Find out num rows satisfying the condition\n",
        "    if aggregation_level == \"recipe\":\n",
        "        number_of_rows_satisfying_condition = (df\n",
        "                                             .filter(condition)\n",
        "                                             .agg(F.countDistinct(\"id\"))).first()[0]\n",
        "\n",
        "        n_rows_total = (df.agg(F.countDistinct(\"id\"))).first()[0]\n",
        "    if aggregation_level == \"review\":\n",
        "        number_of_rows_satisfying_condition = (df\n",
        "                                             .filter(condition)\n",
        "                                             .agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
        "        n_rows_total = (df.agg(F.countDistinct(\"id\",\"user_id\"))).first()[0]\n",
        "\n",
        "    # Find out % rows satisfying the conditon and print a properly formatted output\n",
        "    perc_rows = round(number_of_rows_satisfying_condition * 100/ n_rows_total, 2)\n",
        "    print('Condition String                   : \"{}\"'.format(condition))\n",
        "    print(\"Num {}s Satisfying Condition   : {} [{}%]\".format(aggregation_level.title(), number_of_rows_satisfying_condition, perc_rows))\n",
        "    print(\"Total Num {}s                  : {}\".format(aggregation_level.title(), n_rows_total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bef7d930",
      "metadata": {
        "id": "bef7d930"
      },
      "outputs": [],
      "source": [
        "def add_OHE_columns (df, n_name_list):\n",
        "    \"\"\"\n",
        "    Given a list of tags, creates one hot encoded columns for each tag.\n",
        "\n",
        "    Input\n",
        "    Argument 1: Dataframe in which the function will add the new columns\n",
        "    Argument 2: list of tags\n",
        "\n",
        "    Output\n",
        "    Prints the names of columns that have been added\n",
        "    Returns the modified dataframe\n",
        "    \"\"\"\n",
        "    for name in n_name_list:\n",
        "        df = (df.withColumn(\"has_tag_\"+name, F.when(F.array_contains(df.tags, name), 1).otherwise(0)))\n",
        "        print (\"added column: has_tag_\"+name)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce7153d",
      "metadata": {
        "id": "3ce7153d"
      },
      "source": [
        "## Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c72b3b81",
      "metadata": {
        "id": "c72b3b81"
      },
      "outputs": [],
      "source": [
        "interaction_level_df = spark.read.parquet(\"interaction_level_df_processed_data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5804249a",
      "metadata": {
        "id": "5804249a"
      },
      "source": [
        "## Adding user level average features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f20709b",
      "metadata": {
        "id": "0f20709b"
      },
      "outputs": [],
      "source": [
        "partition = Window.partitionBy(\"user_id\")\n",
        "\n",
        "interaction_level_df = (interaction_level_df\n",
        "                        .withColumn(\"user_avg_rating\",\n",
        "                                    F.avg(F.col(\"rating\")).over(partition))\n",
        "                        .withColumn(\"user_n_ratings\",\n",
        "                                    F.count(F.col(\"rating\")).over(partition))\n",
        "                        .withColumn(\"user_avg_years_betwn_review_and_submission\",\n",
        "                                    F.avg(F.col(\"years_since_submission_on_review_date\")).over(partition))\n",
        "                        .withColumn(\"user_avg_prep_time_recipes_reviewed\",\n",
        "                                    F.avg(F.col(\"minutes\")).over(partition))\n",
        "                        .withColumn(\"user_avg_n_steps_recipes_reviewed\",\n",
        "                                    F.avg(F.col(\"n_steps\")).over(partition))\n",
        "                        .withColumn(\"user_avg_n_ingredients_recipes_reviewed\",\n",
        "                                    F.avg(F.col(\"n_ingredients\")).over(partition)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c16293f2",
      "metadata": {
        "id": "c16293f2"
      },
      "outputs": [],
      "source": [
        "nutrition_cols = ['calories',\n",
        "                  'total_fat_per_100_cal',\n",
        "                  'sugar_per_100_cal',\n",
        "                  'sodium_per_100_cal',\n",
        "                  'protein_per_100_cal',\n",
        "                  'saturated_fat_per_100_cal',\n",
        "                  'carbohydrates_per_100_cal']\n",
        "\n",
        "for nutri_col in nutrition_cols:\n",
        "    interaction_level_df = (interaction_level_df\n",
        "                            .withColumn(\"user_avg_{}_recipes_reviewed\".format(nutri_col),\n",
        "                                        F.avg(F.col(nutri_col)).over(partition)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f0e466",
      "metadata": {
        "id": "22f0e466"
      },
      "outputs": [],
      "source": [
        "# Code check cell\n",
        "# Do not edit cells with assert commands\n",
        "# If an error is shown after running this cell, please recheck your code.\n",
        "\n",
        "assert(round(interaction_level_df.filter('user_id == 601529').select('user_avg_rating').first()[0], 2) == 4.22)\n",
        "assert(interaction_level_df.filter('user_id == 601529').select('user_n_ratings').first()[0] == 27)\n",
        "assert(round(interaction_level_df.filter('user_id == 601529').select('user_avg_years_betwn_review_and_submission').first()[0], 2) == 3.51)\n",
        "assert(interaction_level_df.filter('user_id == 233044').select('user_avg_prep_time_recipes_reviewed').first()[0] == 50.3)\n",
        "assert(interaction_level_df.filter('user_id == 233044').select('user_avg_n_steps_recipes_reviewed').first()[0] == 8.8)\n",
        "assert(interaction_level_df.filter('user_id == 233044').select('user_avg_n_ingredients_recipes_reviewed').first()[0] == 8.2)\n",
        "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_total_fat_per_100_cal_recipes_reviewed').first()[0]) == 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30bf31dd",
      "metadata": {
        "id": "30bf31dd"
      },
      "source": [
        "**More Features:**\n",
        "\n",
        "high_ratings = 5 rating\n",
        "\n",
        "- `user_avg_years_betwn_review_and_submission_high_ratings`\n",
        "- `user_avg_prep_time_recipes_reviewed_high_ratings`\n",
        "- `user_avg_n_steps_recipes_reviewed_high_ratings`\n",
        "- `user_avg_n_ingredients_recipes_reviewed_high_ratings`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8325ef64",
      "metadata": {
        "id": "8325ef64"
      },
      "outputs": [],
      "source": [
        "interaction_level_df = (interaction_level_df\n",
        "                        .withColumn(\"ind_5_rating\",\n",
        "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
        "                                     .otherwise(1))\n",
        "                        .withColumn(\"years_since_submission_on_review_date_5_ratings\",\n",
        "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
        "                                     .otherwise(F.col(\"years_since_submission_on_review_date\")))\n",
        "                        .withColumn(\"minutes_5_ratings\",\n",
        "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
        "                                     .otherwise(F.col(\"minutes\")))\n",
        "                        .withColumn(\"n_steps_5_ratings\",\n",
        "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
        "                                     .otherwise(F.col(\"n_steps\")))\n",
        "                        .withColumn(\"n_ingredients_5_ratings\",\n",
        "                                    F.when(interaction_level_df[\"rating\"] != 5, None)\n",
        "                                     .otherwise(F.col(\"n_ingredients\"))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d77c9f32",
      "metadata": {
        "id": "d77c9f32"
      },
      "outputs": [],
      "source": [
        "partition = Window.partitionBy(\"user_id\")\n",
        "\n",
        "interaction_level_df = (interaction_level_df\n",
        "                        .withColumn(\"user_n_5_ratings\",\n",
        "                                    F.sum(F.col(\"ind_5_rating\")).over(partition))\n",
        "                        .withColumn(\"user_avg_years_betwn_review_and_submission_5_ratings\",\n",
        "                                    F.avg(F.col(\"years_since_submission_on_review_date_5_ratings\")).over(partition))\n",
        "                        .withColumn(\"user_avg_prep_time_recipes_reviewed_5_ratings\",\n",
        "                                    F.avg(F.col(\"minutes_5_ratings\")).over(partition))\n",
        "                        .withColumn(\"user_avg_n_steps_recipes_reviewed_5_ratings\",\n",
        "                                    F.avg(F.col(\"n_steps_5_ratings\")).over(partition))\n",
        "                        .withColumn(\"user_avg_n_ingredients_recipes_reviewed_5_ratings\",\n",
        "                                    F.avg(F.col(\"n_ingredients_5_ratings\")).over(partition)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5761b01",
      "metadata": {
        "id": "c5761b01"
      },
      "outputs": [],
      "source": [
        "for nutri_col in nutrition_cols:\n",
        "    interaction_level_df = (interaction_level_df\n",
        "                            .withColumn(\"{}_5_ratings\".format(nutri_col),\n",
        "                                        F.when(interaction_level_df[\"rating\"] != 5, None)\n",
        "                                         .otherwise(F.col(nutri_col))))\n",
        "    interaction_level_df = (interaction_level_df\n",
        "                            .withColumn(\"user_avg_{}_recipes_reviewed_5_ratings\".format(nutri_col),\n",
        "                                        F.avg(F.col(\"{}_5_ratings\".format(nutri_col))).over(partition)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e189be8b",
      "metadata": {
        "id": "e189be8b"
      },
      "outputs": [],
      "source": [
        "# Check - All rows with ratings should have non-null values in corresponding user_avg_5_ratings columns\n",
        "\n",
        "assert(interaction_level_df\n",
        "       .filter(\"rating == 5\")\n",
        "       .filter(interaction_level_df.user_n_5_ratings.isNull() |\n",
        "               interaction_level_df.user_avg_years_betwn_review_and_submission_5_ratings.isNull() |\n",
        "               interaction_level_df.user_avg_prep_time_recipes_reviewed_5_ratings.isNull() |\n",
        "               interaction_level_df.user_avg_n_steps_recipes_reviewed_5_ratings.isNull() |\n",
        "               interaction_level_df.user_avg_n_ingredients_recipes_reviewed_5_ratings.isNull())\n",
        "       .count() == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6532308e",
      "metadata": {
        "id": "6532308e"
      },
      "outputs": [],
      "source": [
        "# Check values for a given user id\n",
        "\n",
        "assert(interaction_level_df.filter('user_id == 233044').select('user_n_5_ratings').first()[0] == 7)\n",
        "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_years_betwn_review_and_submission_5_ratings').first()[0], 2) == 2.24)\n",
        "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_prep_time_recipes_reviewed_5_ratings').first()[0]) == 46)\n",
        "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_n_steps_recipes_reviewed_5_ratings').first()[0], 2) == 7.29)\n",
        "assert(round(interaction_level_df.filter('user_id == 233044').select('user_avg_n_ingredients_recipes_reviewed_5_ratings').first()[0], 2) == 6.86)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf3d54a",
      "metadata": {
        "id": "0cf3d54a"
      },
      "outputs": [],
      "source": [
        "interaction_level_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f063e652",
      "metadata": {
        "id": "f063e652"
      },
      "source": [
        "## Tags level EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c412427a",
      "metadata": {
        "id": "c412427a"
      },
      "outputs": [],
      "source": [
        "interaction_tag_level_df = interaction_level_df.withColumn('individual_tag',F.explode('tags'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bae8d309",
      "metadata": {
        "id": "bae8d309"
      },
      "outputs": [],
      "source": [
        "tags_ratings_summary = (interaction_tag_level_df\n",
        "                        .groupBy('individual_tag').agg(F.avg('rating').alias('avg_user_rating'),\n",
        "#                                                      F.max('rating').alias('max_user_rating'),\n",
        "#                                                      F.min('rating').alias('min_user_rating'),\n",
        "                                                       F.count('rating').alias('n_user_ratings'),\n",
        "                                                       F.countDistinct('id').alias('n_recipes')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba275d65",
      "metadata": {
        "id": "ba275d65"
      },
      "outputs": [],
      "source": [
        "interactions, recipes  =  interaction_level_df.count(), interaction_level_df.agg(F.countDistinct('id')).first()[0]\n",
        "\n",
        "tags_ratings_summary = (tags_ratings_summary.withColumn(\"in_percent_recipies\", F.col (\"n_recipes\")/F.lit(recipes))\n",
        "                                            .withColumn(\"in_percent_interactions\", F.col (\"n_user_ratings\")/F.lit(interactions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "461c9b73",
      "metadata": {
        "id": "461c9b73"
      },
      "source": [
        "#### 1. Top ```n``` most rated tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e752e5e",
      "metadata": {
        "id": "1e752e5e"
      },
      "outputs": [],
      "source": [
        "tags_ratings_summary.sort(F.col(\"n_user_ratings\").desc()).show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01470f1",
      "metadata": {
        "id": "d01470f1"
      },
      "outputs": [],
      "source": [
        "tags_ratings_summary = tags_ratings_summary.filter(tags_ratings_summary.in_percent_interactions < 0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08aa2bec",
      "metadata": {
        "id": "08aa2bec"
      },
      "outputs": [],
      "source": [
        "top_most_frequent_tags = tags_ratings_summary.sort(F.col(\"n_user_ratings\").desc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5807974d",
      "metadata": {
        "id": "5807974d"
      },
      "outputs": [],
      "source": [
        "get_quantiles(df = top_most_frequent_tags ,\n",
        "              col_name = 'in_percent_interactions',\n",
        "              quantiles_list = [0.01,0.25,0.5, 0.75,0.8,0.85,0.9,0.95, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd54f69d",
      "metadata": {
        "id": "fd54f69d"
      },
      "outputs": [],
      "source": [
        "# keep tags appearing in the top 5 percentile\n",
        "top_most_frequent_tags = top_most_frequent_tags.filter(\"in_percent_interactions > 0.16\")\n",
        "\n",
        "top_most_frequent_tags.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b0c01e",
      "metadata": {
        "id": "f9b0c01e"
      },
      "outputs": [],
      "source": [
        "top_frequent_tags_list = [data[0] for data in top_most_frequent_tags.select('individual_tag').collect()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c0dca9c",
      "metadata": {
        "id": "8c0dca9c"
      },
      "outputs": [],
      "source": [
        "interaction_level_df = add_OHE_columns (interaction_level_df, top_frequent_tags_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07fd6b7",
      "metadata": {
        "id": "b07fd6b7"
      },
      "source": [
        "#### 2.  Bottom ```n``` least rated tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "598a70be",
      "metadata": {
        "id": "598a70be"
      },
      "outputs": [],
      "source": [
        "tags_ratings_summary.sort(F.col(\"n_user_ratings\").asc()).show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "124bcb66",
      "metadata": {
        "id": "124bcb66"
      },
      "source": [
        "#### 3. Top ```n``` rated tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "523ccad9",
      "metadata": {
        "id": "523ccad9"
      },
      "outputs": [],
      "source": [
        "tags_ratings_summary.sort(F.col(\"avg_user_rating\").desc()).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c84e12c0",
      "metadata": {
        "id": "c84e12c0"
      },
      "outputs": [],
      "source": [
        "get_quantiles (tags_ratings_summary, \"n_user_ratings\", quantiles_list = [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ab66d4e",
      "metadata": {
        "id": "7ab66d4e"
      },
      "outputs": [],
      "source": [
        "tags_ratings_summary = tags_ratings_summary.filter(tags_ratings_summary.n_user_ratings > 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82fb811b",
      "metadata": {
        "id": "82fb811b"
      },
      "outputs": [],
      "source": [
        "top_rated_tags_df = tags_ratings_summary.sort(F.col(\"avg_user_rating\").desc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f32843f3",
      "metadata": {
        "id": "f32843f3"
      },
      "outputs": [],
      "source": [
        "get_quantiles(df = top_rated_tags_df ,\n",
        "              col_name = 'avg_user_rating',\n",
        "              quantiles_list = [0.01,0.25,0.5, 0.75,0.8,0.85,0.9,0.95, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa364c22",
      "metadata": {
        "id": "fa364c22"
      },
      "outputs": [],
      "source": [
        "# keep tags above 95 percentile\n",
        "top_rated_tags_df = top_rated_tags_df.filter(\"avg_user_rating > 4.53\")\n",
        "\n",
        "top_rated_tags_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5334a444",
      "metadata": {
        "id": "5334a444"
      },
      "outputs": [],
      "source": [
        "top_rated_tags_list = [data[0] for data in top_rated_tags_df.select('individual_tag').collect()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29a19578",
      "metadata": {
        "id": "29a19578"
      },
      "outputs": [],
      "source": [
        "set(top_frequent_tags_list) & set(top_rated_tags_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0b5f428",
      "metadata": {
        "id": "d0b5f428"
      },
      "outputs": [],
      "source": [
        "all_added_columns_set = set(top_frequent_tags_list).union(set(top_rated_tags_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c79c7f52",
      "metadata": {
        "id": "c79c7f52"
      },
      "outputs": [],
      "source": [
        "interaction_level_df = add_OHE_columns (interaction_level_df, top_rated_tags_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6e60eab",
      "metadata": {
        "id": "e6e60eab"
      },
      "source": [
        "#### 3. Bottom ```n``` rated tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4a95fa",
      "metadata": {
        "id": "3a4a95fa"
      },
      "outputs": [],
      "source": [
        "bottom_rated_tags_df = tags_ratings_summary.sort(F.col(\"avg_user_rating\").asc())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128d531c",
      "metadata": {
        "id": "128d531c"
      },
      "outputs": [],
      "source": [
        "get_quantiles (bottom_rated_tags_df, \"avg_user_rating\", quantiles_list = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94557f95",
      "metadata": {
        "id": "94557f95"
      },
      "outputs": [],
      "source": [
        "bottom_rated_tags_df = bottom_rated_tags_df.filter(\"avg_user_rating < 4.00\")\n",
        "\n",
        "bottom_rated_tags_df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3502beca",
      "metadata": {
        "id": "3502beca"
      },
      "outputs": [],
      "source": [
        "bottom_rated_tags_list = [data[0] for data in bottom_rated_tags_df.select('individual_tag').collect()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9070613",
      "metadata": {
        "id": "b9070613"
      },
      "outputs": [],
      "source": [
        "all_added_columns_set & set(bottom_rated_tags_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4151d37a",
      "metadata": {
        "id": "4151d37a"
      },
      "outputs": [],
      "source": [
        "interaction_level_df =  add_OHE_columns(interaction_level_df, bottom_rated_tags_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4219ed5b",
      "metadata": {
        "id": "4219ed5b"
      },
      "source": [
        "## Final DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f1ebe62",
      "metadata": {
        "id": "7f1ebe62"
      },
      "outputs": [],
      "source": [
        "len(interaction_level_df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28febc12",
      "metadata": {
        "id": "28febc12"
      },
      "outputs": [],
      "source": [
        "interaction_level_df.write.mode(\"overwrite\").parquet(\"interaction_level_df_BDA\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}